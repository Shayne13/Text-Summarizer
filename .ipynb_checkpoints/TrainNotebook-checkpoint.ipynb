{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "To begin, import these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To reload: \n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from parser import *\n",
    "from trainer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folders where files are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputXML = \"sample_train\"\n",
    "# testXMLFolder = \"sample_test\"\n",
    "inputXML = \"sample_train_1\"\n",
    "testXMLFolder = \"sample_test_1\"\n",
    "summaryFolder = \"sample_summary\"\n",
    "bodyFolder = \"sample_body\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse\n",
    "Here is where we read and parse all of the Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
      "STAGE [1] -- PARSING XML -- from sample_train_1 ...\n",
      "  -- Done. Took 0.100056 seconds process time for parsing 1 xml file(s). Writing now ...\n",
      "  -- Done. Took 9.00000000001e-05 seconds to write <=2 summary/body file(s)\n",
      "STAGE [2] -- PROCESSING DATA -- (tokenizing/tagging/stopwords/extracting) ...\n",
      "  -- Done. Took 6.968632 seconds process time for processing 1 document(s)\n"
     ]
    }
   ],
   "source": [
    "documents, labels = parse_xml_folder(inputXML, summaryFolder, bodyFolder, writeOption='none')\n",
    "surfaceFeatures = process_data(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "After parsing, take the documents & labels and then train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE [3] -- FEATURIZING -- (TextRank, LexRank, LDA) ...\n",
      "  -- Done. Took 56.708558 seconds process time to featurize 167 vector(s)\n"
     ]
    }
   ],
   "source": [
    "# This might take awhile... \n",
    "features = featurize(documents, surfaceFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE [4] -- TRAINING MODEL -- Logistic Regression ...\n",
      "\n",
      "  -- Done. Took 0.00897200000009 seconds process time to train 167 data points\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BODY       1.00      0.95      0.97       155\n",
      "    SUMMARY       0.60      1.00      0.75        12\n",
      "\n",
      "avg / total       0.97      0.95      0.96       167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "model, featMatrix, vectorizer = train_classifier_log_reg(features, labels)\n",
    "scores = cross_val_score(model, featMatrix, labels, scoring=\"f1_macro\") # accuracy, f1, log_loss\n",
    "#print model.coef_\n",
    "predictions = model.predict(featMatrix)\n",
    "print \"\"\n",
    "print metrics.classification_report(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE [4] -- TRAINING MODEL -- Gaussian Naive Bayes ...\n",
      "  -- Done. Took 0.00816600000007 seconds process time to train 167 data points\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BODY       1.00      0.08      0.15       155\n",
      "    SUMMARY       0.08      1.00      0.14        12\n",
      "\n",
      "avg / total       0.93      0.15      0.15       167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, featMatrix, vectorizer = train_classifier_gaussian_NB(features, labels)\n",
    "scores = cross_val_score(model, featMatrix, labels, scoring=\"f1_macro\") # accuracy, f1, log_loss\n",
    "#print model.coef_\n",
    "predictions = model.predict(featMatrix)\n",
    "print \"\"\n",
    "print metrics.classification_report(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Then, we test on the test data from the testXML folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
      "STAGE [1] -- PARSING XML -- from sample_test_1 ...\n",
      "  -- Done. Took 0.311815 seconds process time for parsing 5 xml file(s). Writing now ...\n",
      "  -- Done. Took 0.00011399999994 seconds to write <=10 summary/body file(s)\n",
      "STAGE [2] -- PROCESSING DATA -- (tokenizing/tagging/stopwords/extracting) ...\n",
      "  -- Done. Took 28.733085 seconds process time for processing 5 document(s)\n"
     ]
    }
   ],
   "source": [
    "testDocuments, testLabels = parse_xml_folder(testXMLFolder, summaryFolder, bodyFolder, writeOption='none')\n",
    "testSurfaceFeatures = process_data(testDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE [3] -- FEATURIZING -- (TextRank, LexRank, LDA) ...\n",
      "  -- Done. Took 333.463294 seconds process time to featurize 787 vector(s)\n",
      "STAGE [5] -- TESTING -- Logistic Regression ...\n",
      "  -- Done. Took 0.00017600000001 seconds process time to test 787 data points\n",
      "[ 0.67241421  0.57963124  0.77413793]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BODY       0.97      0.82      0.89       711\n",
      "    SUMMARY       0.31      0.75      0.44        76\n",
      "\n",
      "avg / total       0.90      0.81      0.84       787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testFeatures = featurize(testDocuments, testSurfaceFeatures)\n",
    "testFeatureMatrix = vectorizer.transform(testFeatures) # Features = List of counters\n",
    "print evaluate_trained_classifier(model, testFeatureMatrix, testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE Analysis \n",
    "To evaluate against ROUGE, we first construct a summary for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_summary(document):\n",
    "    summary = []\n",
    "    for su in document:\n",
    "        if su.label == 'summary':\n",
    "            summary.append(su.text)\n",
    "        else:\n",
    "            break\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "def extract_top_ranked(document, featureMatrix, num):\n",
    "    predictions = model.predict_proba(featureMatrix)\n",
    "    itr = range(len(document))\n",
    "    topIndexes = nlargest(num, itr, key=lambda i: predictions[i][1])\n",
    "    topSentences = [ document[index].text for index in topIndexes]\n",
    "    return topSentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ROUGE\n",
    "Now, we run ROUGE against the gold summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featIndex = 0\n",
    "rouge_gold_summaries = []\n",
    "rouge_generated_summaries = []\n",
    "for i, tdoc in enumerate(testDocuments):\n",
    "    numSentences = len(tdoc)\n",
    "    rouge_gold_summaries.append(extract_summary(tdoc))\n",
    "    rouge_generated_summaries.append(extract_top_ranked(tdoc, testFeatureMatrix[featIndex:featIndex+numSentences], 10))\n",
    "    featIndex += numSentences\n",
    "\n",
    "#print rouge_gold_summaries\n",
    "#print rouge_generated_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_gold_summaries = {}\n",
    "pickle_generated_summaries = {}\n",
    "for i in range(len(rouge_gold_summaries)):\n",
    "    pickle_gold_summaries[i] = rouge_gold_summaries[i]\n",
    "    pickle_generated_summaries[i] = rouge_generated_summaries[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( pickle_gold_summaries, open( \"pickle_gold_summaries.p\", \"wb\" ) )\n",
    "pickle.dump( pickle_generated_summaries, open( \"pickle_generated_summaries.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the 20th�century, a light-aircraft factory, a college, and a paper mill, along with many smaller enterprises, drove the economy.\n",
      "The city has three sites on the National Register of Historic Places—Memorial Park Site, a significant pre-European archaeological find; Heisey House, a Victorian-era museum; and Water Street District, an area with a mix of 19th- and 20th-century architecture.\n",
      "A television station, Havenscope, and a radio station, WLHU, both managed by students, operate on the university campus.\n",
      "Located near the confluence of the West Branch Susquehanna River and Bald Eagle Creek, it is the principal city of the Lock Haven, Pennsylvania, micropolitan statistical area, itself part of the Williamsport–Lock Haven combined statistical area.\n",
      "An eight-room home, the Heisey House, restored to its mid-19th century appearance, displays Victorian-era collections; it was added to the National Register of Historic Places in 1972 and is home to the Clinton County Historical Society.\n",
      "The city sponsors a festival called Airfest at the airport in the summer, a Halloween parade in October, and a holiday parade in December.\n",
      "Built on a site long favored by pre-European peoples, Lock Haven began in 1833 as a timber town and a haven for loggers, boatmen, and other travelers on the river or the West Branch Canal.\n",
      "The central library for Clinton County is the Annie Halenbake Ross Library in Lock Haven; it has about 130,000�books, subscriptions to periodicals, electronic resources, and other materials.\n",
      "The Piper Aviation Museum exhibits aircraft and aircraft equipment, documents, photographs, and memorabilia related to Piper Aircraft.\n",
      "While industry remains important to the city, about a third of Lock Haven's workforce is employed in education, health care, or social services.\n",
      "0.39272\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from RougeRunner import *\n",
    "\n",
    "rougeResults = []\n",
    "\n",
    "loaded_gold_summaries = pickle.load(open( \"pickle_gold_summaries.p\", \"rb\" ) )\n",
    "loaded_generated_summaries = pickle.load( open( \"pickle_generated_summaries.p\", \"rb\" ) )\n",
    "\n",
    "for summaryIndex in range(len(loaded_gold_summaries)):\n",
    "    gold = loaded_gold_summaries[summaryIndex]\n",
    "    genSum = loaded_generated_summaries[summaryIndex]\n",
    "    rougeResults.append(compareUsingRouge(gold, genSum))\n",
    "        \n",
    "for result in rougeResults:\n",
    "    print result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
