{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from parser import *\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from itertools import izip\n",
    "import scipy\n",
    "# import scipy.spatial.distance\n",
    "from numpy.linalg import svd\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import SelectFpr, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from summanlp_textrank import commons, graph, keywords, pagerank_weighted, \\\n",
    "                  summarizer, textrank, textcleaner, textrank_runtime_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To reload: \n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputXML = \"sample_1\"\n",
    "# inputXML = \"sample_rawXML\"\n",
    "summaryFolder = \"sample_summary\"\n",
    "bodyFolder = \"sample_body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
      "STAGE [1] -- PARSING XML -- from sample_1 ...\n",
      "STAGE [2] -- PROCESSING DATA -- (tokenizing/tagging/stopwords/extracting) ...\n"
     ]
    }
   ],
   "source": [
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "# PARSE:\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "documents, labels = parse_xml_folder(inputXML, summaryFolder, bodyFolder, writeOption='none')\n",
    "surfaceFeatures = process_data(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(documents, surfaceFeatures):\n",
    "    print \"STAGE [3] -- FEATURIZING -- (TextRank, LexRank, LDA) ...\"\n",
    "    features = []\n",
    "    for docIndex, doc in enumerate(documents):\n",
    "        documentFeatures = extract_document_wide_features(doc)\n",
    "        documentFeatures.append(surfaceFeatures[docIndex])\n",
    "        features += [ counter_sum(fl) for fl in izip(*documentFeatures) ]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_document_wide_features(document):\n",
    "    documentFeatures = []\n",
    "\n",
    "    documentFeatures.append(textrank_keyphrase(document))\n",
    "    documentFeatures.append(lexrank_keyphrase(document))\n",
    "    documentFeatures.append(textrank_keyword(document))\n",
    "\n",
    "    return documentFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def counter_sum(counterTuple):\n",
    "    counterSum = Counter()\n",
    "    for ele in counterTuple:\n",
    "        counterSum += ele\n",
    "    return counterSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textrank_keyphrase(text):\n",
    "\n",
    "    # Creates the graph and calculates the similarity coefficient for every pair of nodes.\n",
    "    graph = commons.build_graph([ syntacticUnit for syntacticUnit in text])\n",
    "    summarizer._set_graph_edge_weights(graph)\n",
    "    # Remove all nodes with all edges weights equal to zero.\n",
    "    commons.remove_unreachable_nodes(graph)\n",
    "\n",
    "    # Ranks the tokens using the PageRank algorithm. Returns dict of sentence -> score\n",
    "    pagerank_scores = summarizer._pagerank(graph)\n",
    "\n",
    "    # Adds the summa scores to the sentence objects.\n",
    "    # summarizer._add_scores_to_sentences(sentences, pagerank_scores)\n",
    "\n",
    "    results = []\n",
    "    for su in text:\n",
    "        score = (1-pagerank_scores[su.label, su.index]) if (su.label, su.index) in pagerank_scores.keys() else 0.0\n",
    "        results.append(Counter({ 'TEXTRANK_SCORE': score }))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textrank_keyword(text):\n",
    "    txt = u' '.join([ su.text for su in text ])\n",
    "    # Gets a dict of word -> lemma\n",
    "    tokens = textcleaner.clean_text_by_word(text, 'english')\n",
    "    split_text = list(textcleaner.tokenize_by_word(txt))\n",
    "\n",
    "    # Creates the graph and adds the edges\n",
    "    graph = commons.build_graph(keywords._get_words_for_graph(tokens))\n",
    "    keywords._set_graph_edges(graph, tokens, split_text)\n",
    "    del split_text # It's no longer used\n",
    "    commons.remove_unreachable_nodes(graph)\n",
    "\n",
    "    # # Ranks the tokens using the PageRank algorithm. Returns dict of lemma -> score\n",
    "    pagerank_scores = keywords._pagerank_word(graph)\n",
    "    extracted_lemmas = keywords._extract_tokens(graph.nodes(), pagerank_scores, 0.2, None)\n",
    "    lemmas_to_word = keywords._lemmas_to_words(tokens)\n",
    "    keyWords = keywords._get_keywords_with_score(extracted_lemmas, lemmas_to_word)\n",
    "    # # text.split() to keep numbers and punctuation marks, so separeted concepts are not combined\n",
    "    combined_keywords = keywords._get_combined_keywords(keyWords, txt.split())\n",
    "    kw_scores = keywords._format_results(keyWords, combined_keywords, False, True)\n",
    "\n",
    "    results = [ Counter({ 'TEXTRANK_KEYWORD_SCORE': keyword_mean_score(su.basic, kw_scores) }) for su in text ]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keyword_mean_score(sentence, wordScores):\n",
    "    totalScore = sum([ s for w, s in wordScores if w in sentence ])\n",
    "#     print sentence.split()\n",
    "    if not sentence.split(): return 0.0\n",
    "    return totalScore / len(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexrank_keyphrase(text):\n",
    "    results = []\n",
    "    for i in range(len(text)):\n",
    "        results.append(Counter({ 'LEXRANK_SCORE': 0.0 }))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(features, labels):\n",
    "    print \"STAGE [4] -- TRAINING MODEL -- Logistic Regression ...\"\n",
    "    print '<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>'\n",
    "    vectorizer = DictVectorizer(sparse=False)\n",
    "    feature_matrix = vectorizer.fit_transform(features) # Features = List of counters\n",
    "    mod = LogisticRegression(fit_intercept=True, intercept_scaling=1, class_weight='auto')\n",
    "    mod.fit_transform(feature_matrix, labels)\n",
    "    return mod, feature_matrix, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE [3] -- FEATURIZING -- (TextRank, LexRank, LDA) ...\n",
      "STAGE [4] -- TRAINING MODEL -- Logistic Regression ...\n",
      "<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
      "[[-0.1107776  -0.44076819 -0.51537521 -0.26291182 -0.40936365 -0.65803028\n",
      "   0.8746662   0.47629844  0.38003429 -0.43231561 -0.16282134  0.50850067\n",
      "  -0.28099118  0.5485241   0.11009769 -0.06869526  0.31151657  0.20628777\n",
      "   0.12439831 -0.2521433   0.14399234  1.24799225 -0.08775069 -0.47208916]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BODY       0.96      0.69      0.80       330\n",
      "    SUMMARY       0.17      0.67      0.26        30\n",
      "\n",
      "avg / total       0.89      0.69      0.76       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "# TRAIN:\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "features = featurize(documents, surfaceFeatures)\n",
    "# del surfaceFeatures\n",
    "model, featMatrix, vectorizer = train_classifier(features, labels)\n",
    "\n",
    "scores = cross_val_score(model, featMatrix, labels, scoring=\"f1_macro\") # accuracy, f1, log_loss\n",
    "print model.coef_\n",
    "predictions = model.predict(featMatrix)\n",
    "print metrics.classification_report(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
      "STAGE [1] -- PARSING XML -- from sample_test ...\n",
      "STAGE [2] -- PROCESSING DATA -- (tokenizing/tagging/stopwords/extracting) ...\n"
     ]
    }
   ],
   "source": [
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "# TEST:\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "testXMLFolder = \"sample_test\"\n",
    "testDocuments, testLabels = parse_xml_folder(testXMLFolder, summaryFolder, bodyFolder, writeOption='none')\n",
    "testSurfaceFeatures = process_data(testDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE [3] -- FEATURIZING -- (TextRank, LexRank, LDA) ...\n"
     ]
    }
   ],
   "source": [
    "testFeatures = featurize(testDocuments, testSurfaceFeatures)\n",
    "testFeatureMatrix = vectorizer.transform(testFeatures) # Features = List of counters\n",
    "# del testSurfaceFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_trained_classifier(model, feature_matrix, labels):\n",
    "    \"\"\"Evaluate model, the output of train_classifier, on the test data.\"\"\"\n",
    "    print \"STAGE [5] -- TESTING -- Logistic Regression ...\"\n",
    "    print '<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>'\n",
    "    predictions = model.predict(feature_matrix)\n",
    "    print cross_val_score(model, feature_matrix, labels, scoring=\"f1_macro\")\n",
    "    return metrics.classification_report(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE [5] -- TESTING -- Logistic Regression ...\n",
      "<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
      "[ 0.47271142  0.39633872  0.40424908]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BODY       0.94      0.66      0.78       607\n",
      "    SUMMARY       0.06      0.37      0.11        38\n",
      "\n",
      "avg / total       0.89      0.64      0.74       645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print evaluate_trained_classifier(model, testFeatureMatrix, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_summary(document):\n",
    "    summary = []\n",
    "    for su in document:\n",
    "        if su.label == 'summary':\n",
    "            summary.append(su.text)\n",
    "        else:\n",
    "            break\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "def extract_top_ranked(document, featureMatrix, num):\n",
    "    predictions = model.predict_proba(featureMatrix)\n",
    "    itr = range(len(document))\n",
    "    topIndexes = nlargest(num, itr, key=lambda i: predictions[i][1])\n",
    "    topSentences = [ document[index].text for index in topIndexes]\n",
    "    return topSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featIndex = 0\n",
    "rouge_gold_summaries = []\n",
    "rouge_generated_summaries = []\n",
    "for i, tdoc in enumerate(testDocuments):\n",
    "    numSentences = len(tdoc)\n",
    "    rouge_gold_summaries.append(extract_summary(tdoc))\n",
    "    rouge_generated_summaries.append(extract_top_ranked(tdoc, testFeatureMatrix[featIndex:featIndex+numSentences], 10))\n",
    "    featIndex += numSentences\n",
    "\n",
    "#print rouge_gold_summaries\n",
    "#print rouge_generated_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_gold_summaries = {}\n",
    "pickle_generated_summaries = {}\n",
    "for i in range(len(rouge_gold_summaries)):\n",
    "    pickle_gold_summaries[i] = rouge_gold_summaries[i]\n",
    "    pikcle_generated_summaries = rouge_generated_summaries[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( pickle_gold_summaries, open( \"pickle_gold_summaries.p\", \"wb\" ) )\n",
    "pickle.dump( pickle_generated_summaries, open( \"pickle_generated_summaries.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04454, 0.10869, 0.2028]\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from RougeRunner import *\n",
    "\n",
    "rougeResults = []\n",
    "\n",
    "for summaryIndex in range(len(rouge_gold_summaries)):\n",
    "    gold = rouge_gold_summaries[summaryIndex]\n",
    "    genSum = rouge_generated_summaries[summaryIndex]\n",
    "    rougeResults.append(compareUsingRouge(gold, genSum))\n",
    "    \n",
    "print rougeResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
